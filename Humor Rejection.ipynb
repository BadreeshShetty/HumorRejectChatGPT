{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530ffb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import openai\n",
    "from pdf2image import convert_from_bytes\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# https://stackoverflow.com/questions/43314559/extracting-text-from-a-pdf-all-pages-and-output-file-using-python\n",
    "pdf_file = 'Badreesh_Shetty_Resume.pdf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8893205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_page(file, page_number):\n",
    "    pdfReader = PdfReader(file)\n",
    "    page = pdfReader.pages[page_number]\n",
    "    return page.extract_text()\n",
    "\n",
    "def clean_resume(pdf_str):\n",
    "    applicant_resume = re.sub('\\s[,.]', ',', pdf_str)\n",
    "    applicant_resume = re.sub('[\\n]+', '\\n', applicant_resume)\n",
    "    applicant_resume = re.sub('[\\s]+', ' ', applicant_resume)\n",
    "\n",
    "    return applicant_resume\n",
    "\n",
    "\n",
    "def get_response(prompt, system = '', temperature = 0.5):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt},\n",
    "                   {\"role\": \"system\", \"content\": system}],\n",
    "        temperature = temperature\n",
    "    ) \n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc3afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e07753",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfReader = PdfReader(pdf_file)\n",
    "page_numbers = list(range(1, len(pdfReader.pages)+1))\n",
    "# selected_page = st.selectbox(\"Select a page\", page_numbers)\n",
    "selected_page = 0\n",
    "\n",
    "\n",
    "pdf_str=read_pdf_page(pdf_file, selected_page)\n",
    "\n",
    "applicant_resume=clean_resume(pdf_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b008af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a system prompt\n",
    "system_prompt = '''You are a helpful AI Job assistant, provide the answer in a json format.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05dbfa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"basic_info\": {\n",
      "    \"first_name\": \"Badreesh\",\n",
      "    \"last_name\": \"Shetty\",\n",
      "    \"full_name\": \"Badreesh Shetty\",\n",
      "    \"email\": \"badreeshshetty@gmail.com\",\n",
      "    \"phone_number\": \"617-971-7369\",\n",
      "    \"portfolio_website_url\": \"badreeshshetty.github.io\",\n",
      "    \"linkedin_url\": \"linkedin.com/in/badreesh-shetty\",\n",
      "    \"github_main_page_url\": \"badreeshshetty.github.io\",\n",
      "    \"university\": \"Northeastern University\",\n",
      "    \"education_level\": \"Master's\",\n",
      "    \"graduation_year\": 2023,\n",
      "    \"graduation_month\": \"December\",\n",
      "    \"majors\": \"Business Analytics\",\n",
      "    \"GPA\": null,\n",
      "    \"total_years_experience_with_months\": \"3 years and 5 months\"\n",
      "  },\n",
      "  \"work_experience\": [\n",
      "    {\n",
      "      \"job_title\": \"Data Scientist\",\n",
      "      \"company\": \"Centelon\",\n",
      "      \"location\": \"Mumbai, India\",\n",
      "      \"duration\": \"01/2020 - 05/2022\",\n",
      "      \"job_summary\": \"Executed an AWS Python-based ETL pipeline for a real-time OCR tool, handling 30,000+ financial documents (100 GB+) from email attachments for a leading pharmaceutical client. Realized a 48% decrease in manual effort, resulting in a monthly time savings of 50 hours. Led a team of 3 data scientists in conceptualization and execution of a fuzzy string-matching algorithm that improved accuracy and speed of key information extraction resulting in a 17% decrease in processing time. Built the backend infrastructure using Django REST API framework and SQL which facilitated client's access to a CRUD web-app functionality through monthly reports, delivering valuable analytics for data-driven decision-making. Utilized Plotly-Dash and Excel to create interactive visualizations and dashboards, improving stakeholders' comprehension of key findings. Collaborated with cross-functional teams to maintain the codebase and established streamlined continuous integration using Git for efficient project development and version control.\"\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Machine Learning Intern\",\n",
      "      \"company\": \"AAIC Technologies\",\n",
      "      \"location\": \"Mumbai, India\",\n",
      "      \"duration\": \"12/2018 - 04/2019\",\n",
      "      \"job_summary\": \"Designed Sentiment Analysis for Amazon Fine Food Reviews using SVM, K-Nearest Neighbors, and ensemble methods (Random Forest, Gradient Boosting), performed GridSearchCV to find the optimized model with 0.96 F1 score in Gradient Boosting. Performed classification machine learning models for Human Activity recognition using Logistic Regression, Decision Tree, LSTM, and Random Forest resulted in an 83% accuracy on test data.\"\n",
      "    }\n",
      "  ],\n",
      "  \"project_experience\": [\n",
      "    {\n",
      "      \"project_name\": \"Demand Sales Forecasting for Thermofisher (Capstone)\",\n",
      "      \"project_description\": \"Forecasted sales demand for Thermos Fisher's Biosciences division using a hybrid approach, integrating regression, time series, and deep learning models, yielding a 18% MAPE. Integrated flu patterns, stock market data, healthcare ETF stocks, and sentiment data from Google News as feature engineering, enhancing the predictive capabilities and overall robustness of the developed models.\"\n",
      "    },\n",
      "    {\n",
      "      \"project_name\": \"Recommender System for Student Housing in Boston\",\n",
      "      \"project_description\": \"Created a geospatial web app and a Tableau dashboard, utilizing scraped data from Zillow and Analyze Boston, simplifying property and event searches for Boston students by zipcode.\"\n",
      "    }\n",
      "  ],\n",
      "  \"technical_skills\": [\n",
      "    \"Python\",\n",
      "    \"R\",\n",
      "    \"Django\",\n",
      "    \"Flask\",\n",
      "    \"FastAPI\",\n",
      "    \"PostgreSQL\",\n",
      "    \"MongoDB\",\n",
      "    \"Redis\",\n",
      "    \"Snowflake\",\n",
      "    \"AWS\",\n",
      "    \"Alteryx\",\n",
      "    \"Databricks\",\n",
      "    \"Airflow\",\n",
      "    \"Kafka\",\n",
      "    \"Cassandra\",\n",
      "    \"Airbyte\",\n",
      "    \"Redshift\",\n",
      "    \"BigQuery\",\n",
      "    \"Hive\",\n",
      "    \"PowerBI\",\n",
      "    \"Pandas\",\n",
      "    \"Keras\",\n",
      "    \"Scikit-Learn\",\n",
      "    \"Pytorch\",\n",
      "    \"Pyspark\",\n",
      "    \"Matplotlib\",\n",
      "    \"Seaborn\",\n",
      "    \"Docker\",\n",
      "    \"Streamlit\",\n",
      "    \"DBT\",\n",
      "    \"Cron\",\n",
      "    \"PCA\",\n",
      "    \"Kmeans Clustering\",\n",
      "    \"Naive Bayes\",\n",
      "    \"Decision Tree\",\n",
      "    \"Xgboost\",\n",
      "    \"CNN\",\n",
      "    \"RNN\"\n",
      "  ],\n",
      "  \"intrapersonal_skills\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "message_prompt =str(applicant_resume)+'''\n",
    "Summarize the text below into a JSON with exactly the following structure \n",
    "{basic_info: {first_name, last_name, full_name, email, phone_number, portfolio_website_url, \n",
    "linkedin_url, github_main_page_url, university, education_level (BS, MS, or PhD), graduation_year, \n",
    "graduation_month, majors, GPA, total_years_experience_with_months}, \n",
    "work_experience: [{job_title, company, location, duration, job_summary}], \n",
    "project_experience:[{project_name, project_description}], \n",
    "technical_skills:[elaborate_skills_mentioned_throughout_in_resume],\n",
    "intrapersonal_skills: if intrapersonal skills then [elaborate_skills_mentioned_throughout_in_resume] \n",
    "else null}.'''\n",
    " \n",
    "## This code will send the request and display the response\n",
    "gpt_response_resume = get_response(message_prompt,system_prompt)\n",
    "print(gpt_response_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724cd6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': 'Badreesh',\n",
       " 'last_name': 'Shetty',\n",
       " 'full_name': 'Badreesh Shetty',\n",
       " 'email': 'badreeshshetty@gmail.com',\n",
       " 'phone_number': '617-971-7369',\n",
       " 'portfolio_website_url': 'badreeshshetty.github.io',\n",
       " 'linkedin_url': 'linkedin.com/in/badreesh-shetty',\n",
       " 'github_main_page_url': 'badreeshshetty.github.io',\n",
       " 'university': 'Northeastern University',\n",
       " 'education_level': \"Master's\",\n",
       " 'graduation_year': 2023,\n",
       " 'graduation_month': 'December',\n",
       " 'majors': 'Business Analytics',\n",
       " 'GPA': None,\n",
       " 'total_years_experience_with_months': '3 years and 5 months'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "gpt_response_resume_json = json.loads(gpt_response_resume)\n",
    "# print(gpt_response_resume_json)\n",
    "gpt_response_resume_json['basic_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f213d4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: Data Scientist\n",
      "Company: Centelon\n",
      "Location: Mumbai, India\n",
      "Duration: 01/2020 - 05/2022\n",
      "Job Summary: Executed an AWS Python-based ETL pipeline for a real-time OCR tool, handling 30,000+ financial documents (100 GB+) from email attachments for a leading pharmaceutical client. Realized a 48% decrease in manual effort, resulting in a monthly time savings of 50 hours. Led a team of 3 data scientists in conceptualization and execution of a fuzzy string-matching algorithm that improved accuracy and speed of key information extraction resulting in a 17% decrease in processing time. Built the backend infrastructure using Django REST API framework and SQL which facilitated client's access to a CRUD web-app functionality through monthly reports, delivering valuable analytics for data-driven decision-making. Utilized Plotly-Dash and Excel to create interactive visualizations and dashboards, improving stakeholders' comprehension of key findings. Collaborated with cross-functional teams to maintain the codebase and established streamlined continuous integration using Git for efficient project development and version control.\n",
      "\n",
      "\n",
      "Job Title: Machine Learning Intern\n",
      "Company: AAIC Technologies\n",
      "Location: Mumbai, India\n",
      "Duration: 12/2018 - 04/2019\n",
      "Job Summary: Designed Sentiment Analysis for Amazon Fine Food Reviews using SVM, K-Nearest Neighbors, and ensemble methods (Random Forest, Gradient Boosting), performed GridSearchCV to find the optimized model with 0.96 F1 score in Gradient Boosting. Performed classification machine learning models for Human Activity recognition using Logistic Regression, Decision Tree, LSTM, and Random Forest resulted in an 83% accuracy on test data.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " for n,experience in enumerate(gpt_response_resume_json['work_experience']):\n",
    "        if gpt_response_resume_json['work_experience'][n]:\n",
    "            print(\"Job Title:\", experience[\"job_title\"])\n",
    "            print(\"Company:\", experience[\"company\"])\n",
    "            print(\"Location:\", experience[\"location\"])\n",
    "            print(\"Duration:\", experience[\"duration\"])\n",
    "            print(\"Job Summary:\", experience[\"job_summary\"])\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477b63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: Data Scientist\n",
      "Company: Centelon\n",
      "Location: Mumbai, India\n",
      "Duration: 01/2020 - 05/2022\n",
      "Job Summary: Executed an AWS Python-based ETL pipeline for a real-time OCR tool, handling 30,000+ financial documents (100 GB+) from email attachments for a leading pharmaceutical client. Realized a 48% decrease in manual effort, resulting in a monthly time savings of 50 hours. Led a team of 3 data scientists in conceptualization and execution of a fuzzy string-matching algorithm that improved accuracy and speed of key information extraction resulting in a 17% decrease in processing time. Built the backend infrastructure using Django REST API framework and SQL which facilitated client's access to a CRUD web-app functionality through monthly reports, delivering valuable analytics for data-driven decision-making. Utilized Plotly-Dash and Excel to create interactive visualizations and dashboards, improving stakeholders' comprehension of key findings. Collaborated with cross-functional teams to maintain the codebase and established streamlined continuous integration using Git for efficient project development and version control.\n",
      "\n",
      "\n",
      "Job Title: Machine Learning Intern\n",
      "Company: AAIC Technologies\n",
      "Location: Mumbai, India\n",
      "Duration: 12/2018 - 04/2019\n",
      "Job Summary: Designed Sentiment Analysis for Amazon Fine Food Reviews using SVM, K-Nearest Neighbors, and ensemble methods (Random Forest, Gradient Boosting), performed GridSearchCV to find the optimized model with 0.96 F1 score in Gradient Boosting. Performed classification machine learning models for Human Activity recognition using Logistic Regression, Decision Tree, LSTM, and Random Forest resulted in an 83% accuracy on test data.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for experience in gpt_response_resume_json['work_experience']:\n",
    "    print(\"Job Title:\", experience[\"job_title\"])\n",
    "    print(\"Company:\", experience[\"company\"])\n",
    "    print(\"Location:\", experience[\"location\"])\n",
    "    print(\"Duration:\", experience[\"duration\"])\n",
    "    print(\"Job Summary:\", experience[\"job_summary\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a16d27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# try:\n",
    "#     parsed_response = json.loads(gpt_response)\n",
    "#     print(json.dumps(parsed_response, indent=2))  # Display the formatted JSON\n",
    "# except json.JSONDecodeError as e:\n",
    "#     print(f\"Error decoding JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0ce42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description='''\n",
    "About the job\n",
    "Cox Communications is seeking a Senior Data Scientist for the Analytics + Intelligence Team (A+I). This role will report to a Manager of Data Science and will be responsible for architecture and development of analytics applications developed by the Operations & Technology Analytics team. This role will play a central role in the ongoing design and development of key decision-making analytics applications that are deeply integrated into both customer facing and employee facing platforms throughout operations.\n",
    "\n",
    "What You'll Do\n",
    "\n",
    " Analytics Application Architecture:  Identify or design repeatable patterns for implementing analytics application use cases.\n",
    "\n",
    " Plan and document key logic paths, service architecture, and data stores for new use cases\n",
    " Review of existing services for maintainability, scalability, and opportunities for complexity reduction\n",
    " Design with a bias towards observability, reusability, testability, and scale\n",
    " Design of test plans with a preference for automation where possible\n",
    "\n",
    " Development  : Develop analytics applications using industry best practices for design, development, and testing. Make recommendations where needed for tools, services, and ways of working.\n",
    "\n",
    " Minimum \n",
    "\n",
    " Bachelor's degree in a related discipline and 4 years' experience in a related field. The right candidate could also have a different combination, such as a master's degree and 2 years' experience; a Ph.D. and up to 1 year of experience; or 16 years' experience in a related field\n",
    " Strong programming skills and ability to utilize a variety of data/analytic/software languages and tools. (ex: Python, SQL, Docker, Java, etc.)\n",
    " Experience with analytics application design and development.\n",
    " Good communication skills. The ability to comprehend and communicate advanced technical topics to non-technical business partners.\n",
    " Experience with source code version control software, like Git or Bitbucket, and CICD pipelines like Jenkins.\n",
    " Experience designing and implementing large-scale, high data or user volume solutions in cloud platforms such as Google, Microsoft, or Amazon.\n",
    "\n",
    "Preferred\n",
    "\n",
    " Degree in computer science, engineering or closely related field.\n",
    " Knowledge and experience with AWS Services including Lambda, Step function, CloudWatch, EventBridge, EC2, EMR, RedShift, SQS, RDS, IAM, API Gateway.\n",
    " Experience within the telecommunications industry, cable industry or consulting.\n",
    " Experience with data visualization solutions and data visualization tools.\n",
    " Experience with microservice development (Docker, Kubernetes , etc.).\n",
    " Experience in automated test design and implementation\n",
    " Experience in data store selection for large-scale applications (high data volume and/or high read/write)\n",
    "\n",
    "About Cox Communications\n",
    "\n",
    "Cox Communications is the largest private telecom company in America, serving six million homes and businesses. That's a lot, but we also proudly serve our employees. Our benefits and our award-winning culture are just two of the things that make Cox a coveted place to work. If you're interested in bringing people closer through broadband, smart home tech and more, join Cox Communications today!\n",
    "\n",
    "About Cox\n",
    "\n",
    "Cox empowers employees to build a better future and has been doing so for over 120 years. With exciting investments and innovations across transportation, communications, cleantech and healthcare, our family of businesses - which includes Cox Automotive and Cox Communications - is forging a better future for us all. Ready to make your mark? Join us today!\n",
    "\n",
    "Benefits of working at Cox may include health care insurance (medical, dental, vision), retirement planning (401(k)), and paid days off (sick leave, parental leave, flexible vacation/wellness days, and/or PTO). For more details on what benefits you may be offered, visit our benefits page .\n",
    "\n",
    "Cox is an Equal Employment Opportunity employer - All qualified applicants/employees will receive consideration for employment without regard to that individual's age, race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender, gender identity, physical or mental disability, veteran status, genetic information, ethnicity, citizenship, or any other characteristic protected by law.\n",
    "\n",
    "Statement to ALL Third-Party Agencies and Similar Organizations: Cox accepts resumes only from agencies with which we formally engage their services. Please do not forward resumes to our applicant tracking system, Cox employees, Cox hiring manager, or send to any Cox facility. Cox is not responsible for any fees or charges associated with unsolicited resumes.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99794a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_prompt =str(job_description)+'''\n",
    "Summarize the text below into a JSON with exactly the following structure \n",
    "{basic_info: {company_name, job_role, required_experience_for_the_role,qualifications_for_the_role}},\n",
    "technical_skills:[skills_mentioned_in_the_job_application],\n",
    "intrapersonal_skills: if intrapersonal skills then [skills_mentioned_in_the_job_application] \n",
    "else null}.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b707e861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"basic_info\": {\n",
      "    \"company_name\": \"Cox Communications\",\n",
      "    \"job_role\": \"Senior Data Scientist\",\n",
      "    \"required_experience_for_the_role\": \"Bachelor's degree in a related discipline and 4 years' experience in a related field. The right candidate could also have a different combination, such as a master's degree and 2 years' experience; a Ph.D. and up to 1 year of experience; or 16 years' experience in a related field\",\n",
      "    \"qualifications_for_the_role\": \"Strong programming skills and ability to utilize a variety of data/analytic/software languages and tools. (ex: Python, SQL, Docker, Java, etc.), experience with analytics application design and development, good communication skills, experience with source code version control software, experience designing and implementing large-scale, high data or user volume solutions in cloud platforms\"\n",
      "  },\n",
      "  \"technical_skills\": [\n",
      "    \"Python\",\n",
      "    \"SQL\",\n",
      "    \"Docker\",\n",
      "    \"Java\",\n",
      "    \"Git\",\n",
      "    \"Bitbucket\",\n",
      "    \"Jenkins\",\n",
      "    \"Google Cloud\",\n",
      "    \"Microsoft Azure\",\n",
      "    \"Amazon Web Services (AWS)\"\n",
      "  ],\n",
      "  \"intrapersonal_skills\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## This code will send the request and display the response\n",
    "gpt_response_jd = get_response(message_prompt,system_prompt)\n",
    "print(gpt_response_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef20b716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'R',\n",
       " 'Django',\n",
       " 'Flask',\n",
       " 'FastAPI',\n",
       " 'PostgreSQL',\n",
       " 'MongoDB',\n",
       " 'Redis',\n",
       " 'Snowflake',\n",
       " 'AWS',\n",
       " 'Alteryx',\n",
       " 'Databricks',\n",
       " 'Airflow',\n",
       " 'Kafka',\n",
       " 'Cassandra',\n",
       " 'Airbyte',\n",
       " 'Redshift',\n",
       " 'BigQuery',\n",
       " 'Hive',\n",
       " 'PowerBI',\n",
       " 'Pandas',\n",
       " 'Keras',\n",
       " 'Scikit-Learn',\n",
       " 'Pytorch',\n",
       " 'Pyspark',\n",
       " 'Matplotlib',\n",
       " 'Seaborn',\n",
       " 'Docker',\n",
       " 'Streamlit',\n",
       " 'DBT',\n",
       " 'Cron',\n",
       " 'PCA',\n",
       " 'Kmeans Clustering',\n",
       " 'Naive Bayes',\n",
       " 'Decision Tree',\n",
       " 'Xgboost',\n",
       " 'CNN',\n",
       " 'RNN']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_response_resume_json['technical_skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43e7faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_prompt ='''\n",
    "Based on the technical skills mentioned on the resume here:'''+str(gpt_response_resume)+''' and technical skills \\\n",
    "mentioned on the job description here:'''+str(gpt_response_jd)+''' Create a 100 score around where skills \n",
    "where present in both are shown and not present in job description is shown'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2da6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_tech_skills=get_response(message_prompt,system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77030363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"skills_match_score\": 100,\n",
      "  \"matched_skills\": [\n",
      "    \"Python\",\n",
      "    \"SQL\",\n",
      "    \"Docker\"\n",
      "  ],\n",
      "  \"unmatched_skills\": [\n",
      "    \"R\",\n",
      "    \"Django\",\n",
      "    \"Flask\",\n",
      "    \"FastAPI\",\n",
      "    \"PostgreSQL\",\n",
      "    \"MongoDB\",\n",
      "    \"Redis\",\n",
      "    \"Snowflake\",\n",
      "    \"AWS\",\n",
      "    \"Alteryx\",\n",
      "    \"Databricks\",\n",
      "    \"Airflow\",\n",
      "    \"Kafka\",\n",
      "    \"Cassandra\",\n",
      "    \"Airbyte\",\n",
      "    \"Redshift\",\n",
      "    \"BigQuery\",\n",
      "    \"Hive\",\n",
      "    \"PowerBI\",\n",
      "    \"Pandas\",\n",
      "    \"Keras\",\n",
      "    \"Scikit-Learn\",\n",
      "    \"Pytorch\",\n",
      "    \"Pyspark\",\n",
      "    \"Matplotlib\",\n",
      "    \"Seaborn\",\n",
      "    \"Streamlit\",\n",
      "    \"DBT\",\n",
      "    \"Cron\",\n",
      "    \"PCA\",\n",
      "    \"Kmeans Clustering\",\n",
      "    \"Naive Bayes\",\n",
      "    \"Decision Tree\",\n",
      "    \"Xgboost\",\n",
      "    \"CNN\",\n",
      "    \"RNN\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(comparison_tech_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2bc1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
